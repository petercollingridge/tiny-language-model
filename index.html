<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Building a Tiny Language Model</title>
    <link rel="stylesheet" href="styles/main.css">
</head>
<body>
    <div class="container article-page">
        <header>
            <h1>Basic Neural Network</h1>
        </header>

        <article class="article">
            <div class="article-meta">
                Published on December 31, 2025 â€¢ By Peter Collingridge
            </div>

            <h1 class="article-title">Basic Neural Network</h1>

            <div class="article-content">
                <p>
                  In these articles I'm going to work through building a AI model to generate sentences in a similar way to large language models (LLMs).
                  In order to understand how LLMs work, I think it's helpful to build a very simple version.
                  I will use a very small set of sentences to train the model, and the model will be a lot smaller itself, so I'm calling it a tiny language model.
                  I hope that by keeping it simple it will be easier to understand how exactly LLMs work in principle.
                </p>

                <p>
                  One way I'm going keep things simple is by using a stripped down version of English.
                  I'm imagining a group of primitive cave people trying to describe their world, and creating an
                  AI that learns to use their language.
                  The language will use English words, but only a very small vocabulary and simple sentence structures.
                </p>

                <div class="highlight-box">
                    <strong>What you'll learn:</strong> Forward propagation, backpropagation, gradient descent, activation functions, and training loops
                </div>

                <h2>Some simple sentences about sheep</h2>

                <p>
                  In the beginning, our cave people look out at the world and see some sheep.
                  They are able to describe them with two sentences.
                  These are the only possible sentences they can say.
                </p>

                <ul>
                  <li>Sheep are herbivores</li>
                  <li>Sheep are slow</li>
                </ul>
                
                <p>
                  Don't worry that 'herbivores' is not a particularly simple word; the actual words don't matter.
                  The point is there are only two possible sentences, and they have simple Subject - Verb - Object structure.
                  Can we make an AI that learns to generate these sentences?
                </p>

                <h2>Tokenisation</h2>

                <p>
                  In order to generate a sentence, we will create an neural network that learns to predict the next word in a sentence.
                  The first step is to split the sentences into tokens.
                  For this simple model, a token is just a word.
                  I've also added a special token to indicate the start or end of a sentence, which will be useful later.
                  I'm using &lt;BR&gt; for this token but it doesn't matter what it is as long as it won't appear in a normal sentence.
                </p>

                <p>The sentences become lists of tokens:</p>
                <ul>
                  <li>&lt;BR&gt;, sheep, are, herbivores, &lt;BR&gt;</li>
                  <li>&lt;BR&gt;, sheep, are, slow, &lt;BR&gt;</li>
                </ul>

                <div class="note-box">
                  In a real LLM, words get broken down into smaller tokens.
                  For example, 'herbivores' might be split into two tokens 'herbivore', and 's',
                  allowing the LLM to more easily understand that 'herbivores' is the plural of 'herbivore'.
                </div>


                <h2>Neural network</h2>

                <p>
                    The purpose of the neural network is to take in one word (token) at a time, and try to predict the next word.
                    So we have one input node for every token and one output node for every token.
                    To start with, we'll create the simplest possible model which is a single matrix mapping the inputs to the outputs.
                </p>

                <p>
                    I won't explain how the neural network learns in detail here, but it involves adjusting the weights of the connections between nodes (values in the matrix)
                    based on errors in its predictions.
                    There are many good explanations online, such as
                    <a href="https://www.youtube.com/watch?v=aircAruvnKk" target="_blank" rel="noopener noreferrer">3Blue1Brown's Neural Networks series</a>.
                    My code for this example is available on Github.
                </p>

                <p>
                    After training on these two sentences, with 10 000 repetitions, the neural network looked like this,
                    where blue indicates a positive weight and red a negative weight.
                </p>

                <div >
                    <img src="../example1/neural_net.svg" alt="Neural network diagram showing input, hidden, and output layers" style="max-width:100%; height:auto;">
                    <div class="caption">
                        Figure 1: Simple neural network with input, hidden, and output layers for predicting the next word.
                    </div>
                </div>
                
                <h2>Embeddings</h2>

                <p>
                    If you looks at the weights between the input layer and the hidden layer,
                    you can see they are different for each word, except for 'herbivores' and 'wooly'.
                    This is because both words always appear in the same context (after 'sheep are'),
                    so the neural network has learned to treat them the same.
                </p>

                <ul>
                    <li>&lt;BR&gt;: Blue, Red</li>
                    <li>are: Red, Blue</li>
                    <li>herbivores, wooly: Blue, Blue</li>
                    <li>sheep: Red, Red</li>
                </ul>

                <p>
                    In effect, the network has encoded the words as a vector of two numbers.
                    This is a very simple form of word embedding. Instead of needing a separate representation for each word,
                    we can represent each word as a point in a two-dimensional space.
                    Words that appear in similar contexts end up close together in this space.
                </p>

                <p>Plotting the weights in the two-dimensional space gives us a visual representation of the word embeddings.</p>

                <div>
                    <img src="../example1/embeddings_.svg" alt="Plot of word embeddings in two-dimensional space" style="display: block; margin-left: auto; margin-right: auto;">
                    <div class="caption">
                        Figure 2: Plot of word embeddings in two-dimensional space. Notice that the words 'herbivores' and 'wooly' are in the exact same position.
                    </div>
                </div>

                <p>
                    Each word (token) is represented as a one-hot encoded vector.
                    For example, with our vocabulary of five tokens (&lt;&gt;, sheep, are, herbivores, wooly),
                    the word 'sheep' would be represented as [0, 1, 0, 0, 0].
                    This is a sparse representation, where only one element is 'hot' (1) and the rest are 'cold' (0).
                    The neural network learns to map these one-hot vectors to a smaller dense vectors in the hidden layer,
                    which capture semantic relationships between words.
                    Now sheep can be represented as [-1.60, -4.77] in the two-dimensional embedding space.
                </p>
        </article>

        <footer>
            <p>&copy; 2025 Tiny Language Model</p>
        </footer>
    </div>
</body>
</html>