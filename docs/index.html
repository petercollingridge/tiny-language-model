<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Building a Tiny Language Model</title>
    <link rel="stylesheet" href="styles/main.css">
    <link rel="stylesheet" href="styles/networks.css">
</head>
<body>
    <header>
        <h1>Tiny Language Model</h1>
        <p>Understanding large language models by building a tiny one</p>
    </header>

    <div class="container article-page">
        <article class="article">
            <div class="article-meta">
                <h1 class="article-title">Getting Started</h1>
                Published on December 31, 2025 â€¢ By Peter Collingridge
            </div>

            <nav>
                <ul class="article-nav">
                    <li></li>
                    <li><a href="page2.html">Next</a></li>
                </ul>
            </nav>

            <div class="article-content">
                <h2>Introduction</h2>

                <p>
                    The aim of this series of articles is to understand how large language models (LLMs) work by building a very simple version.
                    I've found many articles and videos explaining now neural nets work, or how LLMs work in theory,
                    but I haven't found any that show exactly how an LLM works.
                    That's probably because real LLMs are so complex that no one really understands exactly what they're doing.
                    I'm hoping if I keep things really simple, I can build a tiny language model that is fully understandable.
                    In particular, I want to understand how attention mechanisms and transformers work, since these are the key innovations that make LLMs so powerful.
                </p>

                <div class="highlight">
                    In this first article, we'll cover tokenisation, a simple neural network, and one-hot encoding.
                </div>

                <h2>Some simple sentences about sheep</h2>

                <p>
                    For this exploration of language models, I'm going to imagine there's a group of primitive cave people
                    trying to describe their world. As they learn to speak, we will create an AI that learns to use their language.
                    The language they use will be a stripped down version of English, that has a very limited vocabulary and simple grammar.
                </p>

                <p>
                  In the beginning, our cave people look out at the world and see some sheep.
                  They describe them with two sentences.
                  These are the only possible sentences they can say.
                </p>

                <ul>
                  <li>Sheep are herbivores</li>
                  <li>Sheep are slow</li>
                </ul>
                
                <p>
                  You could argue that 'herbivores' is not a particularly simple word, but the actual words don't matter.
                  The point is there are only two possible sentences, and they have simple Subject - Verb - Object structure.
                  How can we make an AI that learns to generate these sentences?
                </p>

                <h2>Tokenisation</h2>

                <p>
                  In order to generate a sentence, we will create an neural network that learns to predict the next word in a sentence.
                  The first step is to split the sentences into tokens.
                  For this simple model, a token is just a word.
                  I've also added a special token to indicate the start or end of a sentence, which will be useful later.
                  I'm using &lt;BR&gt; for this token but it doesn't matter what it is as long as it won't appear in a normal sentence.
                </p>

                <p>The sentences become lists of tokens:</p>
                <ul>
                  <li>&lt;BR&gt;, sheep, are, herbivores, &lt;BR&gt;</li>
                  <li>&lt;BR&gt;, sheep, are, slow, &lt;BR&gt;</li>
                </ul>

                <div class="note-box">
                  In a real LLM, words get broken down into smaller tokens.
                  For example, 'herbivores' might be split into two tokens 'herbivore', and 's',
                  allowing the LLM to more easily understand that 'herbivores' is the plural of 'herbivore'.
                </div>


                <h2>Neural network</h2>

                <p>
                    The purpose of the neural network is to take in one word (token) at a time, and try to predict the next word.
                    So we have one input node for every token and one output node for every token.
                    To start with, we'll create the simplest possible model which is a single matrix mapping the inputs to the outputs.
                </p>

                <p>
                    I won't explain how the neural network learns in detail here, but it involves adjusting the weights of the connections between nodes (values in the matrix)
                    based on errors in its predictions.
                    There are many good explanations online, such as
                    <a href="https://www.youtube.com/watch?v=aircAruvnKk" target="_blank" rel="noopener noreferrer">3Blue1Brown's Neural Networks series</a>.
                    My code for this example is available on Github.
                </p>

                <h3>Result</h3>

                <p>
                    After training on these two sentences, with 10 000 repetitions, the neural network is a single matrix
                    that looks like this. I've coloured positive values blue and negative values red.
                </p>

                <div>TODO: make interactive</div>

               <table>
                <thead>
                    <tr>
                        <th rowspan="2" colspan="2"></th>
                        <th colspan="5" class="label label-h">Output</th>
                    </tr>
                    <tr>
                        <th>&lt;BR&gt;</th>
                        <th>are</th>
                        <th>herbivores</th>
                        <th>sheep</th>
                        <th>slow</th>
                    </tr>
                </thead>
                <tbody>
                    <tr><th class="label label-v" rowspan="6"><div class="rotated">Input</div></th></tr>
                    <tr>
                        <th>&lt;BR&gt;</th>
                        <td style="background-color: #ff9999;">-4.0494</td>
                        <td style="background-color: #ff9999;">-3.7335</td>
                        <td style="background-color: #ff9999;">-3.0846</td>
                        <td style="background-color: #9999ff;">4.2455</td>
                        <td style="background-color: #ff9999;">-4.0111</td>
                    </tr>
                    <tr>
                        <th>are</th>
                        <td style="background-color: #ff9999;">-4.2853</td>
                        <td style="background-color: #ff9999;">-4.8309</td>
                        <td style="background-color: #9999ff;">1.3934</td>
                        <td style="background-color: #ff9999;">-5.3217</td>
                        <td style="background-color: #9999ff;">1.4553</td>
                    </tr>
                    <tr>
                        <th>herbivores</th>
                        <td style="background-color: #9999ff;">4.9230</td>
                        <td style="background-color: #ff9999;">-3.4498</td>
                        <td style="background-color: #ff9999;">-2.8003</td>
                        <td style="background-color: #ff9999;">-2.8773</td>
                        <td style="background-color: #ff9999;">-2.8303</td>
                    </tr>
                    <tr>
                        <th>sheep</th>
                        <td style="background-color: #ff9999;">-5.1692</td>
                        <td style="background-color: #9999ff;">3.9837</td>
                        <td style="background-color: #ff9999;">-3.2771</td>
                        <td style="background-color: #ff9999;">-3.3501</td>
                        <td style="background-color: #ff9999;">-2.6211</td>
                    </tr>
                    <tr>
                        <th>slow</th>
                        <td style="background-color: #9999ff;">5.1529</td>
                        <td style="background-color: #ff9999;">-2.4297</td>
                        <td style="background-color: #ff9999;">-2.5776</td>
                        <td style="background-color: #ff9999;">-2.4191</td>
                        <td style="background-color: #ff9999;">-2.9745</td>
                    </tr>
                </tbody>
            </table>

            <p>
                We can draw the neural network as a set of input nodes and output nodes with connections between them.
                If we just draw the positive weights, it looks like this:
            </p>

            <div>TODO: make interactive</div>

            <svg class="svg-img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 400 300" height="300">
                <g class="edges" transform="translate(100, 30)">
                    <line x1="0" y1="0" x2="200" y2="180" />
                    <line x1="0" y1="60" x2="200" y2="120" />
                    <line x1="0" y1="60" x2="200" y2="240" />
                    <line x1="0" y1="120" x2="200" y2="0" />
                    <line x1="0" y1="180" x2="200" y2="60" />
                    <line x1="0" y1="240" x2="200" y2="0" />
                </g>

                <g class="nodes" transform="translate(100, 30)">
                    <circle cx="0" cy="0" r="20" />
                    <circle cx="0" cy="60" r="20" />
                    <circle cx="0" cy="120" r="20" />
                    <circle cx="0" cy="180" r="20" />
                    <circle cx="0" cy="240" r="20" />
                </g>

                <g class="node-label label-left" transform="translate(75, 30)">
                    <text x="0" y="0">&lt;BR&gt;</text>
                    <text x="0" y="60">are</text>
                    <text x="0" y="120">herbivores</text>
                    <text x="0" y="180">sheep</text>
                    <text x="0" y="240">slow</text>
                </g>
                
                <g class="nodes" transform="translate(300, 30)">
                    <circle cx="0" cy="0" r="20" />
                    <circle cx="0" cy="60" r="20" />
                    <circle cx="0" cy="120" r="20" />
                    <circle cx="0" cy="180" r="20" />
                    <circle cx="0" cy="240" r="20" />
                </g>

                <g class="node-label" transform="translate(325, 30)">
                    <text x="0" y="0">&lt;BR&gt;</text>
                    <text x="0" y="60">are</text>
                    <text x="0" y="120">herbivores</text>
                    <text x="0" y="180">sheep</text>
                    <text x="0" y="240">slow</text>
                </g>
            </svg>

            <p>
                To see what the model predicts, we look at the input node,
                then pick one positive connection from it to the output node.
            </p>

            <h3>Matrix view</h3>

            <h2>Conclusion</h2>

            <p>
                We created a simple neural network model that consisted of a single matrix.
                It was able to learn how to generate two simple sentences about sheep.
                In the next article, we'll look at how to represent words with vectors.
            </p>
            </div>
        </article>

        <footer>
            <p>&copy; 2025 Tiny Language Model</p>
        </footer>
    </div>
</body>
</html>