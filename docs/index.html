<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Building a Tiny Language Model</title>
    <link rel="stylesheet" href="styles/main.css">
</head>
<body>
    <header>
        <h1>Tiny Language Model</h1>
        <p>Understanding large language models by building a tiny one</p>
    </header>

    <div class="container article-page">
        <article class="article">
            <div class="article-meta">
                Published on December 31, 2025 â€¢ By Peter Collingridge
            </div>

            <h1 class="article-title">Introduction</h1>

            <div class="article-content">
                <p>
                    The aim of this series of articles is to understand how large language models (LLMs) work by building a very simple version.
                    I've found many articles and videos explaining now neural nets work, or how LLMs work in theory,
                    but I haven't found any that show exactly how an LLM works.
                    That's probably because real LLMs are so complex that no one really understands exactly what they're doing.
                    I'm hoping if I keep things really simple, I can build a tiny language model that is fully understandable.
                    In particular, I want to understand how attention mechanisms and transformers work, since these are the key innovations that make LLMs so powerful.
                </p>

                <h2>Some simple sentences about sheep</h2>

                <p>
                  One way I'm going keep things simple is by using a stripped down version of English.
                  I'm imagining a group of primitive cave people trying to describe their world, and creating an
                  AI that learns to use their language.
                  The language will use English words, but only a very small vocabulary and simple sentence structures.
                </p>

                <p>
                  In the beginning, our cave people look out at the world and see some sheep.
                  They are able to describe them with two sentences.
                  These are the only possible sentences they can say.
                </p>

                <ul>
                  <li>Sheep are herbivores</li>
                  <li>Sheep are slow</li>
                </ul>
                
                <p>
                  Don't worry that 'herbivores' is not a particularly simple word; the actual words don't matter.
                  The point is there are only two possible sentences, and they have simple Subject - Verb - Object structure.
                  Can we make an AI that learns to generate these sentences?
                </p>

                <h2>Tokenisation</h2>

                <p>
                  In order to generate a sentence, we will create an neural network that learns to predict the next word in a sentence.
                  The first step is to split the sentences into tokens.
                  For this simple model, a token is just a word.
                  I've also added a special token to indicate the start or end of a sentence, which will be useful later.
                  I'm using &lt;BR&gt; for this token but it doesn't matter what it is as long as it won't appear in a normal sentence.
                </p>

                <p>The sentences become lists of tokens:</p>
                <ul>
                  <li>&lt;BR&gt;, sheep, are, herbivores, &lt;BR&gt;</li>
                  <li>&lt;BR&gt;, sheep, are, slow, &lt;BR&gt;</li>
                </ul>

                <div class="note-box">
                  In a real LLM, words get broken down into smaller tokens.
                  For example, 'herbivores' might be split into two tokens 'herbivore', and 's',
                  allowing the LLM to more easily understand that 'herbivores' is the plural of 'herbivore'.
                </div>


                <h2>Neural network</h2>

                <p>
                    The purpose of the neural network is to take in one word (token) at a time, and try to predict the next word.
                    So we have one input node for every token and one output node for every token.
                    To start with, we'll create the simplest possible model which is a single matrix mapping the inputs to the outputs.
                </p>

                <p>
                    I won't explain how the neural network learns in detail here, but it involves adjusting the weights of the connections between nodes (values in the matrix)
                    based on errors in its predictions.
                    There are many good explanations online, such as
                    <a href="https://www.youtube.com/watch?v=aircAruvnKk" target="_blank" rel="noopener noreferrer">3Blue1Brown's Neural Networks series</a>.
                    My code for this example is available on Github.
                </p>

                <p>
                    After training on these two sentences, with 10 000 repetitions, the neural network looked like this,
                    where blue indicates a positive weight and red a negative weight.
                </p>

               <table>
                <thead>
                    <tr>
                        <th rowspan="2" colspan="2"></th>
                        <th colspan="5" class="label label-h">Output</th>
                    </tr>
                    <tr>
                        <th>&lt;BR&gt;</th>
                        <th>are</th>
                        <th>herbivores</th>
                        <th>sheep</th>
                        <th>slow</th>
                    </tr>
                </thead>
                <tbody>
                    <tr><th class="label label-v" rowspan="6"><div class="rotated">Input</div></th></tr>
                    <tr>
                        <th>&lt;BR&gt;</th>
                        <td style="background-color: #ff9999;">-4.0494</td>
                        <td style="background-color: #ff9999;">-3.7335</td>
                        <td style="background-color: #ff9999;">-3.0846</td>
                        <td style="background-color: #9999ff;">4.2455</td>
                        <td style="background-color: #ff9999;">-4.0111</td>
                    </tr>
                    <tr>
                        <th>are</th>
                        <td style="background-color: #ff9999;">-4.2853</td>
                        <td style="background-color: #ff9999;">-4.8309</td>
                        <td style="background-color: #9999ff;">1.3934</td>
                        <td style="background-color: #ff9999;">-5.3217</td>
                        <td style="background-color: #9999ff;">1.4553</td>
                    </tr>
                    <tr>
                        <th>herbivores</th>
                        <td style="background-color: #9999ff;">4.9230</td>
                        <td style="background-color: #ff9999;">-3.4498</td>
                        <td style="background-color: #ff9999;">-2.8003</td>
                        <td style="background-color: #ff9999;">-2.8773</td>
                        <td style="background-color: #ff9999;">-2.8303</td>
                    </tr>
                    <tr>
                        <th>sheep</th>
                        <td style="background-color: #ff9999;">-5.1692</td>
                        <td style="background-color: #9999ff;">3.9837</td>
                        <td style="background-color: #ff9999;">-3.2771</td>
                        <td style="background-color: #ff9999;">-3.3501</td>
                        <td style="background-color: #ff9999;">-2.6211</td>
                    </tr>
                    <tr>
                        <th>slow</th>
                        <td style="background-color: #9999ff;">5.1529</td>
                        <td style="background-color: #ff9999;">-2.4297</td>
                        <td style="background-color: #ff9999;">-2.5776</td>
                        <td style="background-color: #ff9999;">-2.4191</td>
                        <td style="background-color: #ff9999;">-2.9745</td>
                    </tr>
                </tbody>
            </table>
        </article>

        <footer>
            <p>&copy; 2025 Tiny Language Model</p>
        </footer>
    </div>
</body>
</html>